{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset link: https://drive.google.com/drive/folders/1n67sVTTzye4jtLfk8n-sa2fH2gTx5Ywt?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-25 15:22:34.366213: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-25 15:22:41.957141: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-25 15:22:49.998720: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-25 15:22:49.999036: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-25 15:22:49.999070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "DATA_DIR = \"../Dataset Korosi/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## resep yang nemu di: https://www.kaggle.com/code/vortexkol/alexnet-cnn-architecture-on-tensorflow-beginner\n",
    "\n",
    "# model_alexnet = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Conv2D(filters=128, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(150,150,3)),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.MaxPool2D(pool_size=(3,3)),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "\n",
    "#     tf.keras.layers.Flatten(),\n",
    "\n",
    "#     tf.keras.layers.Dense(1024,activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.5),\n",
    "#     tf.keras.layers.Dense(1024,activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.5),\n",
    "#     tf.keras.layers.Dense(3,activation='softmax')  \n",
    "# ])\n",
    "\n",
    "## resep yang nemu di: https://medium.com/swlh/alexnet-with-tensorflow-46f366559ce8\n",
    "def create_model(size):\n",
    "\n",
    "    model_alexnet = tf.keras.models.Sequential()\n",
    "    model_alexnet.add(tf.keras.layers.experimental.preprocessing.Resizing(size, size, interpolation=\"bilinear\", input_shape=(size,size,3)))\n",
    "    model_alexnet.add(tf.keras.layers.Conv2D(96, 11, strides=4, padding='same'))\n",
    "    model_alexnet.add(tf.keras.layers.Lambda(tf.nn.local_response_normalization))\n",
    "    model_alexnet.add(tf.keras.layers.Activation('relu'))\n",
    "    model_alexnet.add(tf.keras.layers.MaxPooling2D(3, strides=2))\n",
    "    model_alexnet.add(tf.keras.layers.Conv2D(256, 5, strides=4, padding='same'))\n",
    "    model_alexnet.add(tf.keras.layers.Lambda(tf.nn.local_response_normalization))\n",
    "    model_alexnet.add(tf.keras.layers.Activation('relu'))\n",
    "    model_alexnet.add(tf.keras.layers.MaxPooling2D(3, strides=2))\n",
    "    model_alexnet.add(tf.keras.layers.Conv2D(384, 3, strides=4, padding='same'))\n",
    "    model_alexnet.add(tf.keras.layers.Activation('relu'))\n",
    "    model_alexnet.add(tf.keras.layers.Conv2D(384, 3, strides=4, padding='same'))\n",
    "    model_alexnet.add(tf.keras.layers.Activation('relu'))\n",
    "    model_alexnet.add(tf.keras.layers.Conv2D(256, 3, strides=4, padding='same'))\n",
    "    model_alexnet.add(tf.keras.layers.Activation('relu'))\n",
    "    model_alexnet.add(tf.keras.layers.Flatten())\n",
    "    model_alexnet.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "    model_alexnet.add(tf.keras.layers.Dropout(0.5))\n",
    "    model_alexnet.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "    model_alexnet.add(tf.keras.layers.Dropout(0.5))\n",
    "    model_alexnet.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    return model_alexnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-25 15:23:11.506623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-25 15:23:12.365159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-25 15:23:12.365685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-25 15:23:12.383480: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-25 15:23:12.386283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-25 15:23:12.387030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-25 15:23:12.387656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-25 15:23:17.951709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-25 15:23:17.973702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-25 15:23:17.974484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-25 15:23:17.974987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4264 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing (Resizing)         (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 56, 56, 96)        34944     \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 56, 56, 96)        0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 56, 56, 96)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 256)         614656    \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 3, 3, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 1, 384)         885120    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 1, 1, 384)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 384)         1327488   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1, 1, 384)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 1, 1, 256)         884992    \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 1, 1, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              1052672   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 12291     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,593,475\n",
      "Trainable params: 21,593,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_alexnet = create_model(224) ##alexnet terbaik di 224\n",
    "model_alexnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"alexnet_pengujian_learningrate.csv\", \"w\")\n",
    "f.write(\"learningrate,loss,val_loss,acc,val_acc\")\n",
    "f.close()\n",
    "\n",
    "f = open(\"alexnet_pengujian_learningrate_model_summary.txt\", \"w\")\n",
    "f.write(\"--------==== hasil model summary dari percobaan learning rate ====--------\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 304 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# buat data pipeline\n",
    "size = 224\n",
    "training_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2\n",
    "        )\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        \n",
    "    )\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(DATA_DIR, batch_size=16,\n",
    "                                                            class_mode='categorical',\n",
    "                                                            target_size=(size,size))   \n",
    "val_generator = train_generator = training_datagen.flow_from_directory(DATA_DIR, \n",
    "                                                            subset='validation', \n",
    "                                                            batch_size=16,\n",
    "                                                            class_mode='categorical',\n",
    "                                                            target_size=(size,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-25 15:24:00.066693: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8300\n",
      "2022-12-25 15:24:09.016366: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 31s - loss: 1544027.3750 - acc: 0.2500 - val_loss: 3280344.2500 - val_acc: 0.2000 - 31s/epoch - 8s/step\n",
      "Epoch 2/50\n",
      "4/4 - 6s - loss: 428653.3750 - acc: 0.4167 - val_loss: 22.8214 - val_acc: 0.3167 - 6s/epoch - 1s/step\n",
      "Epoch 3/50\n",
      "4/4 - 6s - loss: 1577.3099 - acc: 0.3500 - val_loss: 132.0442 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 4/50\n",
      "4/4 - 6s - loss: 265.3061 - acc: 0.5000 - val_loss: 532.9703 - val_acc: 0.3167 - 6s/epoch - 2s/step\n",
      "Epoch 5/50\n",
      "4/4 - 6s - loss: 519.0761 - acc: 0.3500 - val_loss: 186.4614 - val_acc: 0.2000 - 6s/epoch - 2s/step\n",
      "Epoch 6/50\n",
      "4/4 - 6s - loss: 143.5750 - acc: 0.3667 - val_loss: 26.3882 - val_acc: 0.3167 - 6s/epoch - 1s/step\n",
      "Epoch 7/50\n",
      "4/4 - 6s - loss: 71.1449 - acc: 0.3167 - val_loss: 43.3232 - val_acc: 0.3167 - 6s/epoch - 2s/step\n",
      "Epoch 8/50\n",
      "4/4 - 6s - loss: 64.2267 - acc: 0.4500 - val_loss: 17.3806 - val_acc: 0.2000 - 6s/epoch - 2s/step\n",
      "Epoch 9/50\n",
      "4/4 - 6s - loss: 38.1301 - acc: 0.2167 - val_loss: 21.0198 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 10/50\n",
      "4/4 - 6s - loss: 50.9898 - acc: 0.2333 - val_loss: 7.0555 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 11/50\n",
      "4/4 - 6s - loss: 51.8204 - acc: 0.4167 - val_loss: 54.5547 - val_acc: 0.3167 - 6s/epoch - 2s/step\n",
      "Epoch 12/50\n",
      "4/4 - 6s - loss: 95.7095 - acc: 0.3333 - val_loss: 32.0770 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 13/50\n",
      "4/4 - 6s - loss: 103.6745 - acc: 0.3333 - val_loss: 9.2619 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 14/50\n",
      "4/4 - 6s - loss: 85.7399 - acc: 0.3333 - val_loss: 19.0764 - val_acc: 0.3167 - 6s/epoch - 1s/step\n",
      "Epoch 15/50\n",
      "4/4 - 6s - loss: 17981.3809 - acc: 0.4667 - val_loss: 36.5763 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 16/50\n",
      "4/4 - 6s - loss: 8403.2324 - acc: 0.4000 - val_loss: 103.2990 - val_acc: 0.2000 - 6s/epoch - 2s/step\n",
      "Epoch 17/50\n",
      "4/4 - 6s - loss: 217.8326 - acc: 0.2833 - val_loss: 62.8680 - val_acc: 0.3167 - 6s/epoch - 2s/step\n",
      "Epoch 18/50\n",
      "4/4 - 6s - loss: 124.0058 - acc: 0.3667 - val_loss: 55.2801 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 19/50\n",
      "4/4 - 6s - loss: 126.7797 - acc: 0.4000 - val_loss: 85.6805 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 20/50\n",
      "4/4 - 6s - loss: 153.9418 - acc: 0.4333 - val_loss: 146.5283 - val_acc: 0.3167 - 6s/epoch - 2s/step\n",
      "Epoch 21/50\n",
      "4/4 - 6s - loss: 142.7004 - acc: 0.3833 - val_loss: 74.2851 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 22/50\n",
      "4/4 - 6s - loss: 83.4493 - acc: 0.2667 - val_loss: 4.4885 - val_acc: 0.3167 - 6s/epoch - 2s/step\n",
      "Epoch 23/50\n",
      "4/4 - 6s - loss: 23.8863 - acc: 0.4833 - val_loss: 6.4053 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 24/50\n",
      "4/4 - 6s - loss: 21.4567 - acc: 0.3667 - val_loss: 2.2263 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 25/50\n",
      "4/4 - 6s - loss: 16.8576 - acc: 0.3667 - val_loss: 2.3680 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 26/50\n",
      "4/4 - 6s - loss: 13.5071 - acc: 0.3500 - val_loss: 1.2821 - val_acc: 0.3167 - 6s/epoch - 2s/step\n",
      "Epoch 27/50\n",
      "4/4 - 6s - loss: 12.2404 - acc: 0.3167 - val_loss: 1.0618 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 28/50\n",
      "4/4 - 6s - loss: 13.0215 - acc: 0.4667 - val_loss: 1.1005 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 29/50\n",
      "4/4 - 6s - loss: 5.9304 - acc: 0.3833 - val_loss: 1.1100 - val_acc: 0.2000 - 6s/epoch - 2s/step\n",
      "Epoch 30/50\n",
      "4/4 - 6s - loss: 6.6266 - acc: 0.2500 - val_loss: 1.1089 - val_acc: 0.2000 - 6s/epoch - 2s/step\n",
      "Epoch 31/50\n",
      "4/4 - 6s - loss: 3.4877 - acc: 0.2500 - val_loss: 1.0916 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 32/50\n",
      "4/4 - 6s - loss: 2.3584 - acc: 0.5333 - val_loss: 1.0581 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 33/50\n",
      "4/4 - 6s - loss: 2.1788 - acc: 0.3333 - val_loss: 1.0785 - val_acc: 0.3167 - 6s/epoch - 2s/step\n",
      "Epoch 34/50\n",
      "4/4 - 6s - loss: 1.5122 - acc: 0.3167 - val_loss: 1.0625 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 35/50\n",
      "4/4 - 6s - loss: 1.9306 - acc: 0.4167 - val_loss: 1.0503 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 36/50\n",
      "4/4 - 6s - loss: 1.1716 - acc: 0.4833 - val_loss: 1.0450 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 37/50\n",
      "4/4 - 6s - loss: 1.8352 - acc: 0.4833 - val_loss: 1.0414 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 38/50\n",
      "4/4 - 6s - loss: 1.5563 - acc: 0.4500 - val_loss: 2.6177 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 39/50\n",
      "4/4 - 6s - loss: 4.7076 - acc: 0.4667 - val_loss: 1.0401 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 40/50\n",
      "4/4 - 6s - loss: 2.2846 - acc: 0.4333 - val_loss: 1.0422 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 41/50\n",
      "4/4 - 6s - loss: 1.5166 - acc: 0.4667 - val_loss: 1.0419 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 42/50\n",
      "4/4 - 6s - loss: 1.1901 - acc: 0.4667 - val_loss: 1.0404 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 43/50\n",
      "4/4 - 6s - loss: 1.7383 - acc: 0.5167 - val_loss: 1.0388 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 44/50\n",
      "4/4 - 6s - loss: 1.6544 - acc: 0.4667 - val_loss: 1.0386 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 45/50\n",
      "4/4 - 7s - loss: 2.1062 - acc: 0.4667 - val_loss: 1.0385 - val_acc: 0.4833 - 7s/epoch - 2s/step\n",
      "Epoch 46/50\n",
      "4/4 - 6s - loss: 1.5666 - acc: 0.4833 - val_loss: 1.0399 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 47/50\n",
      "4/4 - 6s - loss: 1.8169 - acc: 0.4667 - val_loss: 1.0428 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 48/50\n",
      "4/4 - 6s - loss: 1.3306 - acc: 0.4333 - val_loss: 1.0420 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 49/50\n",
      "4/4 - 6s - loss: 1.1542 - acc: 0.4667 - val_loss: 1.0402 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 50/50\n",
      "4/4 - 6s - loss: 2.7525 - acc: 0.4833 - val_loss: 1.0400 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/50\n",
      "4/4 - 8s - loss: 5.5517 - acc: 0.3333 - val_loss: 2.4730 - val_acc: 0.3167 - 8s/epoch - 2s/step\n",
      "Epoch 2/50\n",
      "4/4 - 6s - loss: 1.4740 - acc: 0.4167 - val_loss: 1.0604 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 3/50\n",
      "4/4 - 6s - loss: 1.0546 - acc: 0.4833 - val_loss: 1.0411 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 4/50\n",
      "4/4 - 6s - loss: 1.0578 - acc: 0.4833 - val_loss: 1.0377 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 5/50\n",
      "4/4 - 6s - loss: 1.0618 - acc: 0.4833 - val_loss: 1.0457 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 6/50\n",
      "4/4 - 6s - loss: 1.0707 - acc: 0.4833 - val_loss: 1.0384 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 7/50\n",
      "4/4 - 6s - loss: 1.0369 - acc: 0.4833 - val_loss: 1.0446 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 8/50\n",
      "4/4 - 6s - loss: 1.0696 - acc: 0.4667 - val_loss: 1.0430 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 9/50\n",
      "4/4 - 6s - loss: 1.0478 - acc: 0.4833 - val_loss: 1.0405 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 10/50\n",
      "4/4 - 7s - loss: 1.0421 - acc: 0.4833 - val_loss: 1.0391 - val_acc: 0.4833 - 7s/epoch - 2s/step\n",
      "Epoch 11/50\n",
      "4/4 - 6s - loss: 1.0499 - acc: 0.4667 - val_loss: 1.0380 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 12/50\n",
      "4/4 - 6s - loss: 1.0742 - acc: 0.5000 - val_loss: 1.0382 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 13/50\n",
      "4/4 - 6s - loss: 1.0704 - acc: 0.4833 - val_loss: 1.0408 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 14/50\n",
      "4/4 - 6s - loss: 1.0455 - acc: 0.4667 - val_loss: 1.0379 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 15/50\n",
      "4/4 - 6s - loss: 1.0891 - acc: 0.4667 - val_loss: 1.0430 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 16/50\n",
      "4/4 - 6s - loss: 1.0861 - acc: 0.4667 - val_loss: 1.0378 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 17/50\n",
      "4/4 - 6s - loss: 1.0524 - acc: 0.4667 - val_loss: 1.0402 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 18/50\n",
      "4/4 - 6s - loss: 1.0627 - acc: 0.5000 - val_loss: 1.0440 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 19/50\n",
      "4/4 - 6s - loss: 1.0587 - acc: 0.4833 - val_loss: 1.0377 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 20/50\n",
      "4/4 - 6s - loss: 1.0596 - acc: 0.5000 - val_loss: 1.0377 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 21/50\n",
      "4/4 - 6s - loss: 1.0286 - acc: 0.5167 - val_loss: 1.0393 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 22/50\n",
      "4/4 - 6s - loss: 1.0434 - acc: 0.4833 - val_loss: 1.0434 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 23/50\n",
      "4/4 - 6s - loss: 1.0383 - acc: 0.5167 - val_loss: 1.0383 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 24/50\n",
      "4/4 - 6s - loss: 1.0720 - acc: 0.4833 - val_loss: 1.0377 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 25/50\n",
      "4/4 - 6s - loss: 1.0430 - acc: 0.4667 - val_loss: 1.0390 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 26/50\n",
      "4/4 - 6s - loss: 1.0496 - acc: 0.5000 - val_loss: 1.0392 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 27/50\n",
      "4/4 - 6s - loss: 1.0645 - acc: 0.4833 - val_loss: 1.0383 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 28/50\n",
      "4/4 - 6s - loss: 1.0500 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 29/50\n",
      "4/4 - 6s - loss: 1.0393 - acc: 0.4833 - val_loss: 1.0387 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 30/50\n",
      "4/4 - 6s - loss: 1.0186 - acc: 0.4833 - val_loss: 1.0381 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 31/50\n",
      "4/4 - 6s - loss: 1.0292 - acc: 0.4833 - val_loss: 1.0418 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 32/50\n",
      "4/4 - 6s - loss: 1.0703 - acc: 0.4833 - val_loss: 1.0384 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 33/50\n",
      "4/4 - 6s - loss: 1.0797 - acc: 0.4667 - val_loss: 1.0393 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 34/50\n",
      "4/4 - 6s - loss: 1.0593 - acc: 0.4667 - val_loss: 1.0415 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 35/50\n",
      "4/4 - 6s - loss: 1.0424 - acc: 0.4833 - val_loss: 1.0405 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 36/50\n",
      "4/4 - 6s - loss: 1.0621 - acc: 0.5167 - val_loss: 1.0391 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 37/50\n",
      "4/4 - 6s - loss: 1.0266 - acc: 0.5000 - val_loss: 1.0379 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 38/50\n",
      "4/4 - 6s - loss: 1.0566 - acc: 0.4833 - val_loss: 1.0387 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 39/50\n",
      "4/4 - 6s - loss: 1.0363 - acc: 0.4833 - val_loss: 1.0387 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 40/50\n",
      "4/4 - 6s - loss: 1.0471 - acc: 0.4833 - val_loss: 1.0381 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 41/50\n",
      "4/4 - 6s - loss: 1.0557 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 42/50\n",
      "4/4 - 6s - loss: 1.0630 - acc: 0.4833 - val_loss: 1.0374 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 43/50\n",
      "4/4 - 6s - loss: 1.0485 - acc: 0.4833 - val_loss: 1.0382 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 44/50\n",
      "4/4 - 6s - loss: 1.0390 - acc: 0.5000 - val_loss: 1.0383 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 45/50\n",
      "4/4 - 6s - loss: 1.0685 - acc: 0.4833 - val_loss: 1.0397 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 46/50\n",
      "4/4 - 6s - loss: 1.0467 - acc: 0.4833 - val_loss: 1.0384 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 47/50\n",
      "4/4 - 6s - loss: 1.0480 - acc: 0.4833 - val_loss: 1.0376 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 48/50\n",
      "4/4 - 6s - loss: 1.0183 - acc: 0.4833 - val_loss: 1.0377 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 49/50\n",
      "4/4 - 6s - loss: 1.0568 - acc: 0.4833 - val_loss: 1.0378 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 50/50\n",
      "4/4 - 6s - loss: 1.0651 - acc: 0.4833 - val_loss: 1.0390 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/50\n",
      "4/4 - 8s - loss: 1.0991 - acc: 0.2667 - val_loss: 1.0709 - val_acc: 0.4833 - 8s/epoch - 2s/step\n",
      "Epoch 2/50\n",
      "4/4 - 6s - loss: 1.0680 - acc: 0.4833 - val_loss: 1.0424 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 3/50\n",
      "4/4 - 6s - loss: 1.0481 - acc: 0.4833 - val_loss: 1.0379 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 4/50\n",
      "4/4 - 6s - loss: 1.0437 - acc: 0.4833 - val_loss: 1.0382 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 5/50\n",
      "4/4 - 6s - loss: 1.0425 - acc: 0.4833 - val_loss: 1.0390 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 6/50\n",
      "4/4 - 6s - loss: 1.0443 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 7/50\n",
      "4/4 - 6s - loss: 1.0415 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 8/50\n",
      "4/4 - 6s - loss: 1.0374 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 9/50\n",
      "4/4 - 6s - loss: 1.0396 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 10/50\n",
      "4/4 - 6s - loss: 1.0358 - acc: 0.4833 - val_loss: 1.0381 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 11/50\n",
      "4/4 - 6s - loss: 1.0385 - acc: 0.4833 - val_loss: 1.0383 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 12/50\n",
      "4/4 - 6s - loss: 1.0380 - acc: 0.4833 - val_loss: 1.0376 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 13/50\n",
      "4/4 - 6s - loss: 1.0422 - acc: 0.4833 - val_loss: 1.0381 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 14/50\n",
      "4/4 - 6s - loss: 1.0582 - acc: 0.4833 - val_loss: 1.0392 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 15/50\n",
      "4/4 - 6s - loss: 1.0459 - acc: 0.4833 - val_loss: 1.0388 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 16/50\n",
      "4/4 - 6s - loss: 1.0463 - acc: 0.4833 - val_loss: 1.0381 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 17/50\n",
      "4/4 - 6s - loss: 1.0450 - acc: 0.4833 - val_loss: 1.0381 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 18/50\n",
      "4/4 - 6s - loss: 1.0377 - acc: 0.4833 - val_loss: 1.0381 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 19/50\n",
      "4/4 - 6s - loss: 1.0424 - acc: 0.4833 - val_loss: 1.0376 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 20/50\n",
      "4/4 - 6s - loss: 1.0486 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 21/50\n",
      "4/4 - 6s - loss: 1.0459 - acc: 0.4833 - val_loss: 1.0376 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 22/50\n",
      "4/4 - 6s - loss: 1.0411 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 23/50\n",
      "4/4 - 6s - loss: 1.0478 - acc: 0.4833 - val_loss: 1.0378 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 24/50\n",
      "4/4 - 6s - loss: 1.0361 - acc: 0.4833 - val_loss: 1.0376 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 25/50\n",
      "4/4 - 6s - loss: 1.0416 - acc: 0.4833 - val_loss: 1.0376 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 26/50\n",
      "4/4 - 6s - loss: 1.0421 - acc: 0.4833 - val_loss: 1.0374 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 27/50\n",
      "4/4 - 6s - loss: 1.0414 - acc: 0.4833 - val_loss: 1.0376 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 28/50\n",
      "4/4 - 6s - loss: 1.0393 - acc: 0.4833 - val_loss: 1.0379 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 29/50\n",
      "4/4 - 6s - loss: 1.0203 - acc: 0.4833 - val_loss: 1.0376 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 30/50\n",
      "4/4 - 6s - loss: 1.0444 - acc: 0.4833 - val_loss: 1.0374 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 31/50\n",
      "4/4 - 6s - loss: 1.0436 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 32/50\n",
      "4/4 - 6s - loss: 1.0331 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 33/50\n",
      "4/4 - 6s - loss: 1.0401 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 34/50\n",
      "4/4 - 6s - loss: 1.0419 - acc: 0.4833 - val_loss: 1.0374 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 35/50\n",
      "4/4 - 6s - loss: 1.0479 - acc: 0.4833 - val_loss: 1.0376 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 36/50\n",
      "4/4 - 6s - loss: 1.0335 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 37/50\n",
      "4/4 - 6s - loss: 1.0390 - acc: 0.4833 - val_loss: 1.0375 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 38/50\n",
      "4/4 - 6s - loss: 1.0309 - acc: 0.4833 - val_loss: 1.0374 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 39/50\n",
      "4/4 - 6s - loss: 1.0285 - acc: 0.4833 - val_loss: 1.0377 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 40/50\n",
      "4/4 - 6s - loss: 1.0496 - acc: 0.4833 - val_loss: 1.0376 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 41/50\n",
      "4/4 - 6s - loss: 1.0420 - acc: 0.4833 - val_loss: 1.0376 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 42/50\n",
      "4/4 - 6s - loss: 1.0505 - acc: 0.4833 - val_loss: 1.0391 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 43/50\n",
      "4/4 - 6s - loss: 1.0399 - acc: 0.4833 - val_loss: 1.0385 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 44/50\n",
      "4/4 - 6s - loss: 1.0370 - acc: 0.4833 - val_loss: 1.0370 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 45/50\n",
      "4/4 - 6s - loss: 1.0437 - acc: 0.4833 - val_loss: 1.0378 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 46/50\n",
      "4/4 - 6s - loss: 1.0316 - acc: 0.4833 - val_loss: 1.0321 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 47/50\n",
      "4/4 - 6s - loss: 1.0317 - acc: 0.4833 - val_loss: 0.9673 - val_acc: 0.6500 - 6s/epoch - 1s/step\n",
      "Epoch 48/50\n",
      "4/4 - 6s - loss: 0.9985 - acc: 0.5667 - val_loss: 0.8418 - val_acc: 0.7167 - 6s/epoch - 1s/step\n",
      "Epoch 49/50\n",
      "4/4 - 6s - loss: 0.8209 - acc: 0.7333 - val_loss: 0.6820 - val_acc: 0.8000 - 6s/epoch - 1s/step\n",
      "Epoch 50/50\n",
      "4/4 - 6s - loss: 0.6333 - acc: 0.7500 - val_loss: 0.3693 - val_acc: 0.8000 - 6s/epoch - 1s/step\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/50\n",
      "4/4 - 7s - loss: 1.0979 - acc: 0.4333 - val_loss: 1.0962 - val_acc: 0.4833 - 7s/epoch - 2s/step\n",
      "Epoch 2/50\n",
      "4/4 - 6s - loss: 1.0952 - acc: 0.4833 - val_loss: 1.0918 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 3/50\n",
      "4/4 - 6s - loss: 1.0909 - acc: 0.4833 - val_loss: 1.0859 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 4/50\n",
      "4/4 - 6s - loss: 1.0830 - acc: 0.4833 - val_loss: 1.0758 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 5/50\n",
      "4/4 - 6s - loss: 1.0700 - acc: 0.4833 - val_loss: 1.0632 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 6/50\n",
      "4/4 - 6s - loss: 1.0764 - acc: 0.4833 - val_loss: 1.0493 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 7/50\n",
      "4/4 - 6s - loss: 1.0542 - acc: 0.5167 - val_loss: 1.0349 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 8/50\n",
      "4/4 - 6s - loss: 1.0357 - acc: 0.5167 - val_loss: 1.0178 - val_acc: 0.4833 - 6s/epoch - 2s/step\n",
      "Epoch 9/50\n",
      "4/4 - 6s - loss: 0.9999 - acc: 0.4833 - val_loss: 0.9944 - val_acc: 0.7667 - 6s/epoch - 1s/step\n",
      "Epoch 10/50\n",
      "4/4 - 6s - loss: 0.9776 - acc: 0.7167 - val_loss: 0.9029 - val_acc: 0.6500 - 6s/epoch - 1s/step\n",
      "Epoch 11/50\n",
      "4/4 - 6s - loss: 0.9257 - acc: 0.5500 - val_loss: 0.8276 - val_acc: 0.7833 - 6s/epoch - 2s/step\n",
      "Epoch 12/50\n",
      "4/4 - 6s - loss: 0.8289 - acc: 0.7500 - val_loss: 0.7122 - val_acc: 0.7167 - 6s/epoch - 2s/step\n",
      "Epoch 13/50\n",
      "4/4 - 6s - loss: 0.6883 - acc: 0.7500 - val_loss: 0.5660 - val_acc: 0.8000 - 6s/epoch - 1s/step\n",
      "Epoch 14/50\n",
      "4/4 - 6s - loss: 0.5486 - acc: 0.8000 - val_loss: 0.4616 - val_acc: 0.8000 - 6s/epoch - 1s/step\n",
      "Epoch 15/50\n",
      "4/4 - 6s - loss: 0.4576 - acc: 0.8000 - val_loss: 0.4188 - val_acc: 0.8000 - 6s/epoch - 1s/step\n",
      "Epoch 16/50\n",
      "4/4 - 6s - loss: 0.3930 - acc: 0.8000 - val_loss: 0.4239 - val_acc: 0.7833 - 6s/epoch - 1s/step\n",
      "Epoch 17/50\n",
      "4/4 - 6s - loss: 0.4200 - acc: 0.8000 - val_loss: 0.4409 - val_acc: 0.7833 - 6s/epoch - 2s/step\n",
      "Epoch 18/50\n",
      "4/4 - 6s - loss: 0.5105 - acc: 0.8000 - val_loss: 0.6720 - val_acc: 0.7333 - 6s/epoch - 2s/step\n",
      "Epoch 19/50\n",
      "4/4 - 6s - loss: 0.5781 - acc: 0.7167 - val_loss: 0.5183 - val_acc: 0.7167 - 6s/epoch - 1s/step\n",
      "Epoch 20/50\n",
      "4/4 - 6s - loss: 0.3977 - acc: 0.7667 - val_loss: 0.3558 - val_acc: 0.8500 - 6s/epoch - 1s/step\n",
      "Epoch 21/50\n",
      "4/4 - 6s - loss: 0.3808 - acc: 0.7833 - val_loss: 0.3439 - val_acc: 0.8167 - 6s/epoch - 1s/step\n",
      "Epoch 22/50\n",
      "4/4 - 6s - loss: 0.3547 - acc: 0.8167 - val_loss: 0.4380 - val_acc: 0.7667 - 6s/epoch - 1s/step\n",
      "Epoch 23/50\n",
      "4/4 - 6s - loss: 0.4089 - acc: 0.8000 - val_loss: 0.4485 - val_acc: 0.7667 - 6s/epoch - 1s/step\n",
      "Epoch 24/50\n",
      "4/4 - 6s - loss: 0.4508 - acc: 0.7833 - val_loss: 0.3481 - val_acc: 0.8167 - 6s/epoch - 1s/step\n",
      "Epoch 25/50\n",
      "4/4 - 6s - loss: 0.5480 - acc: 0.7333 - val_loss: 0.3726 - val_acc: 0.8000 - 6s/epoch - 2s/step\n",
      "Epoch 26/50\n",
      "4/4 - 6s - loss: 0.3693 - acc: 0.8333 - val_loss: 0.7571 - val_acc: 0.7000 - 6s/epoch - 2s/step\n",
      "Epoch 27/50\n",
      "4/4 - 6s - loss: 0.4869 - acc: 0.7667 - val_loss: 0.3601 - val_acc: 0.8167 - 6s/epoch - 2s/step\n",
      "Epoch 28/50\n",
      "4/4 - 6s - loss: 0.4441 - acc: 0.7500 - val_loss: 0.3713 - val_acc: 0.8000 - 6s/epoch - 1s/step\n",
      "Epoch 29/50\n",
      "4/4 - 6s - loss: 0.3672 - acc: 0.8000 - val_loss: 0.3559 - val_acc: 0.8000 - 6s/epoch - 2s/step\n",
      "Epoch 30/50\n",
      "4/4 - 6s - loss: 0.3921 - acc: 0.7833 - val_loss: 0.3684 - val_acc: 0.8167 - 6s/epoch - 1s/step\n",
      "Epoch 31/50\n",
      "4/4 - 6s - loss: 0.3793 - acc: 0.8000 - val_loss: 0.3635 - val_acc: 0.8000 - 6s/epoch - 2s/step\n",
      "Epoch 32/50\n",
      "4/4 - 6s - loss: 0.3800 - acc: 0.8000 - val_loss: 0.3537 - val_acc: 0.8000 - 6s/epoch - 1s/step\n",
      "Epoch 33/50\n",
      "4/4 - 6s - loss: 0.3359 - acc: 0.8167 - val_loss: 0.3425 - val_acc: 0.8167 - 6s/epoch - 2s/step\n",
      "Epoch 34/50\n",
      "4/4 - 6s - loss: 0.3439 - acc: 0.8333 - val_loss: 0.3764 - val_acc: 0.8333 - 6s/epoch - 1s/step\n",
      "Epoch 35/50\n",
      "4/4 - 6s - loss: 0.3688 - acc: 0.8000 - val_loss: 0.3435 - val_acc: 0.8000 - 6s/epoch - 2s/step\n",
      "Epoch 36/50\n",
      "4/4 - 6s - loss: 0.3485 - acc: 0.8167 - val_loss: 0.3341 - val_acc: 0.8167 - 6s/epoch - 2s/step\n",
      "Epoch 37/50\n",
      "4/4 - 6s - loss: 0.3387 - acc: 0.8167 - val_loss: 0.3371 - val_acc: 0.8167 - 6s/epoch - 2s/step\n",
      "Epoch 38/50\n",
      "4/4 - 6s - loss: 0.3507 - acc: 0.8167 - val_loss: 0.3297 - val_acc: 0.8167 - 6s/epoch - 1s/step\n",
      "Epoch 39/50\n",
      "4/4 - 6s - loss: 0.3418 - acc: 0.8333 - val_loss: 0.3349 - val_acc: 0.8167 - 6s/epoch - 1s/step\n",
      "Epoch 40/50\n",
      "4/4 - 6s - loss: 0.3438 - acc: 0.8000 - val_loss: 0.3340 - val_acc: 0.8000 - 6s/epoch - 1s/step\n",
      "Epoch 41/50\n",
      "4/4 - 6s - loss: 0.3385 - acc: 0.8167 - val_loss: 0.3318 - val_acc: 0.8000 - 6s/epoch - 1s/step\n",
      "Epoch 42/50\n",
      "4/4 - 6s - loss: 0.3372 - acc: 0.8000 - val_loss: 0.3243 - val_acc: 0.8333 - 6s/epoch - 2s/step\n",
      "Epoch 43/50\n",
      "4/4 - 6s - loss: 0.3493 - acc: 0.8000 - val_loss: 0.3333 - val_acc: 0.8000 - 6s/epoch - 1s/step\n",
      "Epoch 44/50\n",
      "4/4 - 6s - loss: 0.3465 - acc: 0.8167 - val_loss: 0.3337 - val_acc: 0.8167 - 6s/epoch - 2s/step\n",
      "Epoch 45/50\n",
      "4/4 - 6s - loss: 0.3312 - acc: 0.8333 - val_loss: 0.3145 - val_acc: 0.8167 - 6s/epoch - 2s/step\n",
      "Epoch 46/50\n",
      "4/4 - 6s - loss: 0.3218 - acc: 0.8500 - val_loss: 0.3114 - val_acc: 0.8500 - 6s/epoch - 1s/step\n",
      "Epoch 47/50\n",
      "4/4 - 6s - loss: 0.3369 - acc: 0.8500 - val_loss: 0.3278 - val_acc: 0.8167 - 6s/epoch - 1s/step\n",
      "Epoch 48/50\n",
      "4/4 - 6s - loss: 0.3550 - acc: 0.8333 - val_loss: 0.3277 - val_acc: 0.8167 - 6s/epoch - 1s/step\n",
      "Epoch 49/50\n",
      "4/4 - 6s - loss: 0.3309 - acc: 0.8167 - val_loss: 0.3183 - val_acc: 0.8667 - 6s/epoch - 2s/step\n",
      "Epoch 50/50\n",
      "4/4 - 6s - loss: 0.3280 - acc: 0.8333 - val_loss: 0.3280 - val_acc: 0.8333 - 6s/epoch - 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "OPTIMIZERS = [  [tf.keras.optimizers.Adam(learning_rate=0.1),\"01\"],\n",
    "                [tf.keras.optimizers.Adam(learning_rate=0.01),\"001\"],\n",
    "                [tf.keras.optimizers.Adam(learning_rate=0.001),\"0001\"],\n",
    "                [tf.keras.optimizers.Adam(learning_rate=0.0001),\"00001\"]]\n",
    "\n",
    "\n",
    "for optimizer in OPTIMIZERS:\n",
    "    model_alexnet = create_model(size)\n",
    "    \n",
    "    with open('alexnet_pengujian_learningrate_model_summary.txt', 'w') as f:\n",
    "        f.write(\"\\n Optimizer: \"+str(optimizer[1])+\" \\n\")\n",
    "        with redirect_stdout(f):\n",
    "            model_alexnet.summary()\n",
    "    f.close()\n",
    "\n",
    "    Wsave = model_alexnet.get_weights()\n",
    "    tf.keras.utils.plot_model(model_alexnet,to_file=\"./\"+str(optimizer[1])+\"_alexnet.png\")    \n",
    "    \n",
    "    model_alexnet.set_weights(Wsave)\n",
    "    model_alexnet.compile(  optimizer=optimizer[0], \n",
    "                            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                            metrics=['acc']\n",
    "                            )\n",
    "    history = model_alexnet.fit(train_generator,validation_data=val_generator,epochs=50, verbose = 2)\n",
    "\n",
    "    f = open(\"alexnet_pengujian_learningrate.csv\", \"a\")\n",
    "    f.write(\"\\n\"+str(optimizer[1])+\",\"+\n",
    "        str(history.history['loss'][-1])+\",\"+\n",
    "        str(history.history['val_loss'][-1])+\",\"+\n",
    "        str(history.history['acc'][-1])+\",\"+\n",
    "        str(history.history['val_acc'][-1])\n",
    "    )\n",
    "    f.close()\n",
    "     \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Loss Model')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('./alexnet/loss_'+str(optimizer[1])+'.png')\n",
    "    plt.cla()\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('./alexnet/acc_'+str(optimizer[1])+'.png')\n",
    "    \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_128 (Funct  (None, 4, 4, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 20480)             0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 20480)             0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 512)               10486272  \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,745,795\n",
      "Trainable params: 12,711,683\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_mobilenet(size):\n",
    "\n",
    "    mobilenet = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "    input_shape=(size,size,3),\n",
    "    weights=None,\n",
    "    include_top=False,\n",
    "    )\n",
    "    \n",
    "    model_mobilenet = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer( input_shape=(size,size,3)),\n",
    "    mobilenet,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(512,activation='relu'),\n",
    "    tf.keras.layers.Dense(3,activation='softmax')\n",
    "    ])\n",
    "    return model_mobilenet\n",
    "\n",
    "mobilenet = create_mobilenet(128) #overfit, tapi hasil terbaik ya 128\n",
    "mobilenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"mobilenet_pengujian_learningrate.csv\", \"w\")\n",
    "f.write(\"learningrate,loss,val_loss,acc,val_acc\")\n",
    "f.close()\n",
    "\n",
    "f = open(\"mobilenet_pengujian_learningrate_model_summary.txt\", \"w\")\n",
    "f.write(\"--------==== hasil model summary dari percobaan learningrate ====--------\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 304 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "size = 128    \n",
    "training_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2\n",
    "        )\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        \n",
    "    )\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(DATA_DIR, batch_size=16,\n",
    "                                                            class_mode='categorical',\n",
    "                                                            target_size=(size,size))   \n",
    "val_generator = train_generator = training_datagen.flow_from_directory(DATA_DIR, \n",
    "                                                            subset='validation', \n",
    "                                                            batch_size=16,\n",
    "                                                            class_mode='categorical',\n",
    "                                                            target_size=(size,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/50\n",
      "4/4 - 19s - loss: 5574.7344 - acc: 0.2667 - val_loss: 774.0119 - val_acc: 0.3167 - 19s/epoch - 5s/step\n",
      "Epoch 2/50\n",
      "4/4 - 5s - loss: 151.7171 - acc: 0.3333 - val_loss: 249.2049 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 3/50\n",
      "4/4 - 6s - loss: 1.3027 - acc: 0.4000 - val_loss: 454.1255 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 4/50\n",
      "4/4 - 5s - loss: 3.7866 - acc: 0.4833 - val_loss: 545.2984 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 5/50\n",
      "4/4 - 5s - loss: 0.8799 - acc: 0.4833 - val_loss: 532.8345 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 6/50\n",
      "4/4 - 5s - loss: 1.1795 - acc: 0.4667 - val_loss: 759.8495 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 7/50\n",
      "4/4 - 5s - loss: 0.9714 - acc: 0.4333 - val_loss: 1423.2808 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 8/50\n",
      "4/4 - 5s - loss: 0.9663 - acc: 0.4833 - val_loss: 2236.1633 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 9/50\n",
      "4/4 - 5s - loss: 0.9534 - acc: 0.4833 - val_loss: 3020.5061 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 10/50\n",
      "4/4 - 5s - loss: 0.9660 - acc: 0.4833 - val_loss: 3554.8806 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 11/50\n",
      "4/4 - 5s - loss: 0.9773 - acc: 0.4833 - val_loss: 4024.0100 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 12/50\n",
      "4/4 - 5s - loss: 1.3052 - acc: 0.4833 - val_loss: 4304.0659 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 13/50\n",
      "4/4 - 5s - loss: 0.9698 - acc: 0.4833 - val_loss: 4414.5405 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 14/50\n",
      "4/4 - 5s - loss: 1.0721 - acc: 0.4667 - val_loss: 4436.2007 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 15/50\n",
      "4/4 - 5s - loss: 0.9606 - acc: 0.4833 - val_loss: 4373.8525 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 16/50\n",
      "4/4 - 5s - loss: 0.9773 - acc: 0.4167 - val_loss: 4261.8208 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 17/50\n",
      "4/4 - 5s - loss: 0.9664 - acc: 0.3833 - val_loss: 4120.8877 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 18/50\n",
      "4/4 - 5s - loss: 1.1541 - acc: 0.4333 - val_loss: 3930.0625 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 19/50\n",
      "4/4 - 5s - loss: 1.0099 - acc: 0.4167 - val_loss: 3757.2124 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 20/50\n",
      "4/4 - 5s - loss: 0.9604 - acc: 0.4833 - val_loss: 3584.0916 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 21/50\n",
      "4/4 - 5s - loss: 0.9230 - acc: 0.4833 - val_loss: 3421.6162 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 22/50\n",
      "4/4 - 5s - loss: 0.9732 - acc: 0.4000 - val_loss: 3270.8623 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 23/50\n",
      "4/4 - 5s - loss: 0.9239 - acc: 0.4833 - val_loss: 3123.1614 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 24/50\n",
      "4/4 - 5s - loss: 0.9094 - acc: 0.4833 - val_loss: 2975.6716 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 25/50\n",
      "4/4 - 5s - loss: 0.9253 - acc: 0.4833 - val_loss: 2833.5303 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 26/50\n",
      "4/4 - 5s - loss: 0.9438 - acc: 0.4167 - val_loss: 2703.7078 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 27/50\n",
      "4/4 - 5s - loss: 0.8882 - acc: 0.4833 - val_loss: 2581.7751 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 28/50\n",
      "4/4 - 5s - loss: 0.9306 - acc: 0.4667 - val_loss: 2462.5242 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 29/50\n",
      "4/4 - 5s - loss: 0.9329 - acc: 0.4667 - val_loss: 2356.3840 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 30/50\n",
      "4/4 - 5s - loss: 0.9291 - acc: 0.3500 - val_loss: 2253.5952 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 31/50\n",
      "4/4 - 6s - loss: 0.9317 - acc: 0.5000 - val_loss: 2155.0151 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 32/50\n",
      "4/4 - 5s - loss: 0.9057 - acc: 0.4833 - val_loss: 2061.0466 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 33/50\n",
      "4/4 - 5s - loss: 0.9095 - acc: 0.4333 - val_loss: 1973.5222 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 34/50\n",
      "4/4 - 5s - loss: 3.7388 - acc: 0.4833 - val_loss: 1889.1234 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 35/50\n",
      "4/4 - 5s - loss: 0.9110 - acc: 0.4833 - val_loss: 1790.1383 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 36/50\n",
      "4/4 - 5s - loss: 1.0127 - acc: 0.4833 - val_loss: 1692.2335 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 37/50\n",
      "4/4 - 5s - loss: 1.0474 - acc: 0.4833 - val_loss: 1597.4938 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 38/50\n",
      "4/4 - 5s - loss: 1.0455 - acc: 0.4833 - val_loss: 1528.5922 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 39/50\n",
      "4/4 - 5s - loss: 1.0589 - acc: 0.4833 - val_loss: 1464.8945 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 40/50\n",
      "4/4 - 5s - loss: 1.0481 - acc: 0.4833 - val_loss: 1405.8901 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 41/50\n",
      "4/4 - 5s - loss: 1.0466 - acc: 0.4833 - val_loss: 1349.7814 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 42/50\n",
      "4/4 - 5s - loss: 1.0439 - acc: 0.4833 - val_loss: 1297.0140 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 43/50\n",
      "4/4 - 5s - loss: 1.0404 - acc: 0.4833 - val_loss: 1246.4851 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 44/50\n",
      "4/4 - 5s - loss: 1.0427 - acc: 0.4833 - val_loss: 1198.8286 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 45/50\n",
      "4/4 - 5s - loss: 1.0435 - acc: 0.4833 - val_loss: 1153.6185 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 46/50\n",
      "4/4 - 5s - loss: 1.0416 - acc: 0.4833 - val_loss: 1112.5751 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 47/50\n",
      "4/4 - 5s - loss: 1.0490 - acc: 0.4833 - val_loss: 1071.9802 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 48/50\n",
      "4/4 - 5s - loss: 1.0389 - acc: 0.4833 - val_loss: 1033.3601 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 49/50\n",
      "4/4 - 5s - loss: 1.0455 - acc: 0.4833 - val_loss: 997.1378 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 50/50\n",
      "4/4 - 6s - loss: 1.0460 - acc: 0.4833 - val_loss: 961.2296 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/50\n",
      "4/4 - 10s - loss: 135.7779 - acc: 0.3500 - val_loss: 13.5983 - val_acc: 0.3167 - 10s/epoch - 2s/step\n",
      "Epoch 2/50\n",
      "4/4 - 5s - loss: 39.8997 - acc: 0.5333 - val_loss: 157.8635 - val_acc: 0.2000 - 5s/epoch - 1s/step\n",
      "Epoch 3/50\n",
      "4/4 - 5s - loss: 9.2792 - acc: 0.6500 - val_loss: 449.4549 - val_acc: 0.2000 - 5s/epoch - 1s/step\n",
      "Epoch 4/50\n",
      "4/4 - 5s - loss: 9.7016 - acc: 0.6667 - val_loss: 483.9306 - val_acc: 0.2000 - 5s/epoch - 1s/step\n",
      "Epoch 5/50\n",
      "4/4 - 5s - loss: 3.2446 - acc: 0.7000 - val_loss: 386.9415 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 6/50\n",
      "4/4 - 5s - loss: 1.7558 - acc: 0.7167 - val_loss: 369.5856 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 7/50\n",
      "4/4 - 5s - loss: 2.2208 - acc: 0.7833 - val_loss: 324.5513 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 8/50\n",
      "4/4 - 5s - loss: 4.3387 - acc: 0.6333 - val_loss: 245.9585 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 9/50\n",
      "4/4 - 5s - loss: 0.9230 - acc: 0.5167 - val_loss: 200.4713 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 10/50\n",
      "4/4 - 5s - loss: 1.2055 - acc: 0.4667 - val_loss: 201.5991 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 11/50\n",
      "4/4 - 5s - loss: 1.4763 - acc: 0.5500 - val_loss: 182.1375 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 12/50\n",
      "4/4 - 5s - loss: 1.3041 - acc: 0.4667 - val_loss: 133.8129 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 13/50\n",
      "4/4 - 5s - loss: 0.7635 - acc: 0.6000 - val_loss: 82.8891 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 14/50\n",
      "4/4 - 5s - loss: 0.7407 - acc: 0.6000 - val_loss: 24.3261 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 15/50\n",
      "4/4 - 5s - loss: 1.1361 - acc: 0.5500 - val_loss: 7.6145 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 16/50\n",
      "4/4 - 5s - loss: 0.7782 - acc: 0.6000 - val_loss: 1.8372 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 17/50\n",
      "4/4 - 5s - loss: 1.0423 - acc: 0.6000 - val_loss: 1.1096 - val_acc: 0.2000 - 5s/epoch - 1s/step\n",
      "Epoch 18/50\n",
      "4/4 - 5s - loss: 1.9114 - acc: 0.5667 - val_loss: 1.0840 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 19/50\n",
      "4/4 - 5s - loss: 0.6732 - acc: 0.6167 - val_loss: 1.0840 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 20/50\n",
      "4/4 - 5s - loss: 0.6994 - acc: 0.7333 - val_loss: 1.0839 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 21/50\n",
      "4/4 - 5s - loss: 0.8643 - acc: 0.6000 - val_loss: 1.0851 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 22/50\n",
      "4/4 - 5s - loss: 0.6892 - acc: 0.6500 - val_loss: 1.0867 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 23/50\n",
      "4/4 - 5s - loss: 0.6249 - acc: 0.6333 - val_loss: 1.0891 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 24/50\n",
      "4/4 - 5s - loss: 0.7050 - acc: 0.6667 - val_loss: 1.0937 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 25/50\n",
      "4/4 - 5s - loss: 0.9437 - acc: 0.6833 - val_loss: 1.0977 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 26/50\n",
      "4/4 - 5s - loss: 0.9083 - acc: 0.6500 - val_loss: 1.1107 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 27/50\n",
      "4/4 - 5s - loss: 0.5354 - acc: 0.7000 - val_loss: 1.1232 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 28/50\n",
      "4/4 - 5s - loss: 0.6568 - acc: 0.7000 - val_loss: 1.1376 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 29/50\n",
      "4/4 - 5s - loss: 0.4878 - acc: 0.7500 - val_loss: 1.1484 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 30/50\n",
      "4/4 - 5s - loss: 0.5481 - acc: 0.7000 - val_loss: 1.1578 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 31/50\n",
      "4/4 - 5s - loss: 0.4850 - acc: 0.7333 - val_loss: 1.1857 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 32/50\n",
      "4/4 - 5s - loss: 0.3611 - acc: 0.8333 - val_loss: 1.2221 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 33/50\n",
      "4/4 - 5s - loss: 0.4351 - acc: 0.7667 - val_loss: 1.2578 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 34/50\n",
      "4/4 - 5s - loss: 0.3475 - acc: 0.8000 - val_loss: 1.3107 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 35/50\n",
      "4/4 - 5s - loss: 0.5119 - acc: 0.7667 - val_loss: 1.3337 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 36/50\n",
      "4/4 - 5s - loss: 0.4259 - acc: 0.7833 - val_loss: 1.3734 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 37/50\n",
      "4/4 - 5s - loss: 0.5324 - acc: 0.7500 - val_loss: 1.3653 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 38/50\n",
      "4/4 - 5s - loss: 0.3574 - acc: 0.8000 - val_loss: 1.3033 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 39/50\n",
      "4/4 - 5s - loss: 0.4028 - acc: 0.7667 - val_loss: 1.2512 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 40/50\n",
      "4/4 - 5s - loss: 0.4184 - acc: 0.6833 - val_loss: 1.2249 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 41/50\n",
      "4/4 - 5s - loss: 0.4510 - acc: 0.7167 - val_loss: 1.2591 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 42/50\n",
      "4/4 - 5s - loss: 0.4373 - acc: 0.7667 - val_loss: 1.2985 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 43/50\n",
      "4/4 - 5s - loss: 0.4950 - acc: 0.7500 - val_loss: 1.2930 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 44/50\n",
      "4/4 - 5s - loss: 0.3608 - acc: 0.8000 - val_loss: 1.2579 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 45/50\n",
      "4/4 - 5s - loss: 0.3883 - acc: 0.8000 - val_loss: 1.2897 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 46/50\n",
      "4/4 - 5s - loss: 0.3815 - acc: 0.7667 - val_loss: 1.3350 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 47/50\n",
      "4/4 - 5s - loss: 0.3570 - acc: 0.7833 - val_loss: 1.3422 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 48/50\n",
      "4/4 - 5s - loss: 6.9637 - acc: 0.7333 - val_loss: 1.3913 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 49/50\n",
      "4/4 - 5s - loss: 0.5844 - acc: 0.7000 - val_loss: 1.2745 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "Epoch 50/50\n",
      "4/4 - 5s - loss: 0.8166 - acc: 0.5667 - val_loss: 1.1471 - val_acc: 0.3167 - 5s/epoch - 1s/step\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/50\n",
      "4/4 - 10s - loss: 20.6278 - acc: 0.4833 - val_loss: 1.0984 - val_acc: 0.3167 - 10s/epoch - 2s/step\n",
      "Epoch 2/50\n",
      "4/4 - 5s - loss: 3.1967 - acc: 0.7333 - val_loss: 1.0986 - val_acc: 0.2000 - 5s/epoch - 1s/step\n",
      "Epoch 3/50\n",
      "4/4 - 5s - loss: 2.7813 - acc: 0.7167 - val_loss: 1.0974 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 4/50\n",
      "4/4 - 5s - loss: 2.2190 - acc: 0.7500 - val_loss: 1.0949 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 5/50\n",
      "4/4 - 5s - loss: 0.8840 - acc: 0.7333 - val_loss: 1.0894 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 6/50\n",
      "4/4 - 5s - loss: 0.6207 - acc: 0.8333 - val_loss: 1.0835 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 7/50\n",
      "4/4 - 5s - loss: 0.4723 - acc: 0.7167 - val_loss: 1.0792 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 8/50\n",
      "4/4 - 5s - loss: 0.4317 - acc: 0.7833 - val_loss: 1.0760 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 9/50\n",
      "4/4 - 5s - loss: 0.4359 - acc: 0.8333 - val_loss: 1.0746 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 10/50\n",
      "4/4 - 5s - loss: 0.8656 - acc: 0.7833 - val_loss: 1.0738 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 11/50\n",
      "4/4 - 5s - loss: 0.6353 - acc: 0.6833 - val_loss: 1.0737 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 12/50\n",
      "4/4 - 5s - loss: 0.5136 - acc: 0.7500 - val_loss: 1.0709 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 13/50\n",
      "4/4 - 5s - loss: 0.4577 - acc: 0.7500 - val_loss: 1.0689 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 14/50\n",
      "4/4 - 5s - loss: 0.3189 - acc: 0.8500 - val_loss: 1.0687 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 15/50\n",
      "4/4 - 5s - loss: 0.5204 - acc: 0.7500 - val_loss: 1.0709 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 16/50\n",
      "4/4 - 5s - loss: 0.3439 - acc: 0.8000 - val_loss: 1.0732 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 17/50\n",
      "4/4 - 6s - loss: 0.4474 - acc: 0.6667 - val_loss: 1.0752 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 18/50\n",
      "4/4 - 5s - loss: 0.2713 - acc: 0.8333 - val_loss: 1.0784 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 19/50\n",
      "4/4 - 5s - loss: 0.2610 - acc: 0.8333 - val_loss: 1.0819 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 20/50\n",
      "4/4 - 5s - loss: 0.7773 - acc: 0.7500 - val_loss: 1.0830 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 21/50\n",
      "4/4 - 5s - loss: 0.3262 - acc: 0.8000 - val_loss: 1.0779 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 22/50\n",
      "4/4 - 5s - loss: 0.3100 - acc: 0.8667 - val_loss: 1.0758 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 23/50\n",
      "4/4 - 5s - loss: 0.4036 - acc: 0.8333 - val_loss: 1.0771 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 24/50\n",
      "4/4 - 5s - loss: 0.3350 - acc: 0.8500 - val_loss: 1.0809 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 25/50\n",
      "4/4 - 5s - loss: 0.3756 - acc: 0.8833 - val_loss: 1.0869 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 26/50\n",
      "4/4 - 5s - loss: 0.3346 - acc: 0.8667 - val_loss: 1.0906 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 27/50\n",
      "4/4 - 6s - loss: 0.3349 - acc: 0.8333 - val_loss: 1.0938 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 28/50\n",
      "4/4 - 5s - loss: 0.3067 - acc: 0.8333 - val_loss: 1.0996 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 29/50\n",
      "4/4 - 5s - loss: 0.2838 - acc: 0.8667 - val_loss: 1.1051 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 30/50\n",
      "4/4 - 5s - loss: 0.5176 - acc: 0.8833 - val_loss: 1.1124 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 31/50\n",
      "4/4 - 6s - loss: 0.2091 - acc: 0.8833 - val_loss: 1.1177 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 32/50\n",
      "4/4 - 5s - loss: 0.6924 - acc: 0.7667 - val_loss: 1.1280 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 33/50\n",
      "4/4 - 5s - loss: 0.3004 - acc: 0.8833 - val_loss: 1.1388 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 34/50\n",
      "4/4 - 5s - loss: 0.5253 - acc: 0.8000 - val_loss: 1.1362 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 35/50\n",
      "4/4 - 5s - loss: 0.3544 - acc: 0.8167 - val_loss: 1.1199 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 36/50\n",
      "4/4 - 5s - loss: 0.3840 - acc: 0.8833 - val_loss: 1.1039 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 37/50\n",
      "4/4 - 5s - loss: 1.4066 - acc: 0.7667 - val_loss: 1.0995 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 38/50\n",
      "4/4 - 5s - loss: 0.3958 - acc: 0.8333 - val_loss: 1.1029 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 39/50\n",
      "4/4 - 5s - loss: 0.2470 - acc: 0.8333 - val_loss: 1.1086 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 40/50\n",
      "4/4 - 5s - loss: 0.3881 - acc: 0.8833 - val_loss: 1.1156 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 41/50\n",
      "4/4 - 5s - loss: 0.3284 - acc: 0.8333 - val_loss: 1.1172 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 42/50\n",
      "4/4 - 5s - loss: 0.4739 - acc: 0.8667 - val_loss: 1.1194 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 43/50\n",
      "4/4 - 5s - loss: 0.2898 - acc: 0.8833 - val_loss: 1.1178 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 44/50\n",
      "4/4 - 5s - loss: 0.2754 - acc: 0.8500 - val_loss: 1.1209 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 45/50\n",
      "4/4 - 5s - loss: 0.3913 - acc: 0.9000 - val_loss: 1.1070 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 46/50\n",
      "4/4 - 5s - loss: 0.3195 - acc: 0.8500 - val_loss: 1.0948 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 47/50\n",
      "4/4 - 5s - loss: 0.3690 - acc: 0.8167 - val_loss: 1.0964 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 48/50\n",
      "4/4 - 5s - loss: 0.3094 - acc: 0.8667 - val_loss: 1.1048 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 49/50\n",
      "4/4 - 5s - loss: 0.4768 - acc: 0.8833 - val_loss: 1.1070 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 50/50\n",
      "4/4 - 5s - loss: 0.2778 - acc: 0.8167 - val_loss: 1.1097 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/50\n",
      "4/4 - 9s - loss: 1.6171 - acc: 0.4833 - val_loss: 1.0981 - val_acc: 0.4833 - 9s/epoch - 2s/step\n",
      "Epoch 2/50\n",
      "4/4 - 5s - loss: 0.7190 - acc: 0.6833 - val_loss: 1.0965 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 3/50\n",
      "4/4 - 5s - loss: 0.6331 - acc: 0.8000 - val_loss: 1.0948 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 4/50\n",
      "4/4 - 5s - loss: 0.5451 - acc: 0.8000 - val_loss: 1.0935 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 5/50\n",
      "4/4 - 5s - loss: 0.7161 - acc: 0.7333 - val_loss: 1.0916 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 6/50\n",
      "4/4 - 5s - loss: 0.4843 - acc: 0.7667 - val_loss: 1.0901 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 7/50\n",
      "4/4 - 5s - loss: 0.5222 - acc: 0.7833 - val_loss: 1.0879 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 8/50\n",
      "4/4 - 5s - loss: 0.5943 - acc: 0.7667 - val_loss: 1.0871 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 9/50\n",
      "4/4 - 5s - loss: 0.5689 - acc: 0.7667 - val_loss: 1.0856 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 10/50\n",
      "4/4 - 5s - loss: 0.6614 - acc: 0.7833 - val_loss: 1.0838 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 11/50\n",
      "4/4 - 5s - loss: 0.6164 - acc: 0.7167 - val_loss: 1.0834 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 12/50\n",
      "4/4 - 5s - loss: 0.4166 - acc: 0.7667 - val_loss: 1.0824 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 13/50\n",
      "4/4 - 5s - loss: 0.5693 - acc: 0.6833 - val_loss: 1.0809 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 14/50\n",
      "4/4 - 5s - loss: 0.5315 - acc: 0.7667 - val_loss: 1.0768 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 15/50\n",
      "4/4 - 5s - loss: 0.5534 - acc: 0.7000 - val_loss: 1.0737 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 16/50\n",
      "4/4 - 5s - loss: 0.5179 - acc: 0.8000 - val_loss: 1.0756 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 17/50\n",
      "4/4 - 5s - loss: 0.4963 - acc: 0.8000 - val_loss: 1.0776 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 18/50\n",
      "4/4 - 5s - loss: 0.4204 - acc: 0.7667 - val_loss: 1.0760 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 19/50\n",
      "4/4 - 5s - loss: 0.3437 - acc: 0.8667 - val_loss: 1.0727 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 20/50\n",
      "4/4 - 5s - loss: 0.4409 - acc: 0.8167 - val_loss: 1.0695 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 21/50\n",
      "4/4 - 5s - loss: 0.5401 - acc: 0.7333 - val_loss: 1.0686 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 22/50\n",
      "4/4 - 5s - loss: 0.4770 - acc: 0.7667 - val_loss: 1.0709 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 23/50\n",
      "4/4 - 5s - loss: 0.4347 - acc: 0.7833 - val_loss: 1.0678 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 24/50\n",
      "4/4 - 5s - loss: 0.9208 - acc: 0.6500 - val_loss: 1.0597 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 25/50\n",
      "4/4 - 5s - loss: 0.5460 - acc: 0.8000 - val_loss: 1.0597 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 26/50\n",
      "4/4 - 5s - loss: 0.4558 - acc: 0.8167 - val_loss: 1.0628 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 27/50\n",
      "4/4 - 5s - loss: 0.3614 - acc: 0.8500 - val_loss: 1.0643 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 28/50\n",
      "4/4 - 5s - loss: 0.3953 - acc: 0.8167 - val_loss: 1.0653 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 29/50\n",
      "4/4 - 5s - loss: 0.3156 - acc: 0.8833 - val_loss: 1.0630 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 30/50\n",
      "4/4 - 5s - loss: 0.2698 - acc: 0.8500 - val_loss: 1.0602 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 31/50\n",
      "4/4 - 5s - loss: 0.3913 - acc: 0.8000 - val_loss: 1.0577 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 32/50\n",
      "4/4 - 5s - loss: 0.4566 - acc: 0.7667 - val_loss: 1.0566 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 33/50\n",
      "4/4 - 5s - loss: 0.4454 - acc: 0.8167 - val_loss: 1.0576 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 34/50\n",
      "4/4 - 5s - loss: 0.3539 - acc: 0.8333 - val_loss: 1.0588 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 35/50\n",
      "4/4 - 5s - loss: 0.4637 - acc: 0.7500 - val_loss: 1.0547 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 36/50\n",
      "4/4 - 5s - loss: 0.4121 - acc: 0.8000 - val_loss: 1.0538 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 37/50\n",
      "4/4 - 5s - loss: 0.4215 - acc: 0.8500 - val_loss: 1.0548 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 38/50\n",
      "4/4 - 6s - loss: 0.6087 - acc: 0.7333 - val_loss: 1.0520 - val_acc: 0.4833 - 6s/epoch - 1s/step\n",
      "Epoch 39/50\n",
      "4/4 - 5s - loss: 0.4931 - acc: 0.8000 - val_loss: 1.0501 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 40/50\n",
      "4/4 - 5s - loss: 0.3473 - acc: 0.8167 - val_loss: 1.0500 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 41/50\n",
      "4/4 - 5s - loss: 0.3462 - acc: 0.7833 - val_loss: 1.0495 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 42/50\n",
      "4/4 - 5s - loss: 0.3405 - acc: 0.8667 - val_loss: 1.0503 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 43/50\n",
      "4/4 - 5s - loss: 0.3350 - acc: 0.7833 - val_loss: 1.0502 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 44/50\n",
      "4/4 - 5s - loss: 0.3933 - acc: 0.8167 - val_loss: 1.0472 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 45/50\n",
      "4/4 - 5s - loss: 0.3937 - acc: 0.8500 - val_loss: 1.0460 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 46/50\n",
      "4/4 - 5s - loss: 0.5007 - acc: 0.7333 - val_loss: 1.0476 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 47/50\n",
      "4/4 - 5s - loss: 0.5385 - acc: 0.7667 - val_loss: 1.0461 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 48/50\n",
      "4/4 - 5s - loss: 0.3659 - acc: 0.8167 - val_loss: 1.0476 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 49/50\n",
      "4/4 - 5s - loss: 0.3362 - acc: 0.8833 - val_loss: 1.0505 - val_acc: 0.4833 - 5s/epoch - 1s/step\n",
      "Epoch 50/50\n",
      "4/4 - 5s - loss: 0.3350 - acc: 0.8333 - val_loss: 1.0498 - val_acc: 0.4833 - 5s/epoch - 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OPTIMIZERS = [  [tf.keras.optimizers.Adam(learning_rate=0.1),\"01\"],\n",
    "                [tf.keras.optimizers.Adam(learning_rate=0.01),\"001\"],\n",
    "                [tf.keras.optimizers.Adam(learning_rate=0.001),\"0001\"],\n",
    "                [tf.keras.optimizers.Adam(learning_rate=0.0001),\"00001\"]]\n",
    "\n",
    "for optimizer in OPTIMIZERS:\n",
    "    model_alexnet = create_mobilenet(size)\n",
    "    \n",
    "    with open('mobilenet_pengujian_learningrate_model_summary.txt', 'w') as f:\n",
    "        f.write(\"\\n Optimizer: \"+str(optimizer[1])+\" \\n\")\n",
    "        with redirect_stdout(f):\n",
    "            model_alexnet.summary()\n",
    "    f.close()\n",
    "\n",
    "    Wsave = model_alexnet.get_weights()\n",
    "    tf.keras.utils.plot_model(model_alexnet,to_file=\"./\"+str(optimizer[1])+\"_mobilenet.png\")    \n",
    "    \n",
    "    model_alexnet.set_weights(Wsave)\n",
    "    model_alexnet.compile(  optimizer=optimizer[0], \n",
    "                            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                            metrics=['acc']\n",
    "                            )\n",
    "    history = model_alexnet.fit(train_generator,validation_data=val_generator,epochs=50, verbose = 2)\n",
    "\n",
    "    f = open(\"mobilenet_pengujian_learningrate.csv\", \"a\")\n",
    "    f.write(\"\\n\"+str(optimizer[1])+\",\"+\n",
    "        str(history.history['loss'][-1])+\",\"+\n",
    "        str(history.history['val_loss'][-1])+\",\"+\n",
    "        str(history.history['acc'][-1])+\",\"+\n",
    "        str(history.history['val_acc'][-1])\n",
    "    )\n",
    "    f.close()\n",
    "     \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Loss Model')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('./mobilenet/loss_'+str(optimizer[1])+'.png')\n",
    "    plt.cla()\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('./mobilenet/acc_'+str(optimizer[1])+'.png')\n",
    "    \n",
    "    plt.clf()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "863388641bf8518efc9f9c13edbd8c5a9191560a3a7c90c25b6128b1c338502d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "220b59355a8dadb4ba3bf77e4f9773cad1e54b32290f811d9d316f35385cbe26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
